{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18 - CIFAR100 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = CIFAR100('../data/', train=True, transform=Compose([Resize((224, 224)), ToTensor()]))\n",
    "cifar_ldr = DataLoader(cifar100)\n",
    "\n",
    "# get a batch of data\n",
    "x, y = next(iter(cifar_ldr))\n",
    "x, y = x.to(0), y.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet = resnet18()\n",
    "_ = resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tacklebox.hook_management import HookManager\n",
    "hookmngr = HookManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward hook function signature: (module, inputs, output)\n",
    "def print_shape(module, inputs, output):\n",
    "    print('%s output shape: ' % module.name, end='')\n",
    "    print(output.shape)\n",
    "\n",
    "module = resnet.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register print_shape with the module, naming it myconv for reference\n",
    "hookmngr.register_forward_hook(print_shape, hook_fn_name='print_shape', myconv=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tacklebox.hook_management.HookFunction object at 0x7f94a1058978>\n",
      "<print_shape[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (active)>\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": [
    "# lookup the HookFunction wrapper for print_shape\n",
    "print(hookmngr.name_to_hookfn['print_shape'])\n",
    "# lookup the HookHandle wrapper for the handle returned from registering print_shape with myconv\n",
    "print(hookmngr.name_to_hookhandle['print_shape[myconv]'])\n",
    "# lookup the module named myconv\n",
    "print(hookmngr.name_to_module['myconv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now register the same method with another module, this time leaving it deactivated for now\n",
    "hookmngr.register_forward_hook(print_shape, mylayer=resnet.layer1, activate=False)\n",
    "\n",
    "# note that we didnt need to name the hook function again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<print_shape[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "[<print_shape[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (active)>, <print_shape[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>]\n"
     ]
    }
   ],
   "source": [
    "# lookup the HookHandle wrapper for the handle returned from registering print_shape with mylayer\n",
    "print(hookmngr.name_to_hookhandle['print_shape[mylayer]'])\n",
    "# lookup our new module, mylayer\n",
    "print(hookmngr.name_to_module['mylayer'])\n",
    "# lookup all HookHandles corresponding to the print_shape hook function\n",
    "print(hookmngr.name_to_hookfn['print_shape'].handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myconv output shape: torch.Size([1, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# lets test the hook function\n",
    "with torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myconv output shape: torch.Size([1, 64, 112, 112])\n",
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# activate mylayer\n",
    "hookmngr.activate_module_hooks_by_name('mylayer')\n",
    "\n",
    "with torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate hook registered to myconv\n",
    "hookmngr.deactivate_all_hooks()\n",
    "\n",
    "with torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# use python context to activate hook registered to mylayer, then deactivate it after forward pass\n",
    "with torch.no_grad():\n",
    "    with hookmngr.hook_module_context_by_name('mylayer'):\n",
    "        resnet(x)\n",
    "    \n",
    "    resnet(x)  # hook doesnt execute once we exit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myconv output shape: torch.Size([1, 64, 112, 112])\n",
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# now try combining the contexts -- less indentation :)\n",
    "with hookmngr.hook_all_context() + torch.no_grad():\n",
    "    resnet(x)\n",
    "    print(torch.is_grad_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with multiple hook functions\n",
    "\n",
    "# forward pre-hook function signature: (module, inputs)\n",
    "def zero_input(module, inputs):\n",
    "    input, = inputs\n",
    "    input = input - input\n",
    "    print('Set %s input to zero' % module.name)\n",
    "    return input\n",
    "\n",
    "def print_mean(module, inputs, output):\n",
    "    print('%s input mean = %.2f, output mean = %.2f' % (module.name, inputs[0].mean().item(),\n",
    "                                                        output.mean().item()))\n",
    "    \n",
    "# to pass modules without names, pass as args instead of as kwargs\n",
    "hookmngr.register_forward_pre_hook(zero_input, resnet.conv1, resnet.layer1,\n",
    "                                   hook_fn_name='zero_input', activate=False)\n",
    "hookmngr.register_forward_hook(print_mean, resnet.conv1, resnet.layer1,\n",
    "                              hook_fn_name='print_mean', activate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set myconv input to zero\n",
      "myconv output shape: torch.Size([1, 64, 112, 112])\n",
      "myconv input mean = 0.00, output mean = 0.00\n",
      "Set mylayer input to zero\n",
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n",
      "mylayer input mean = 0.00, output mean = 0.00\n"
     ]
    }
   ],
   "source": [
    "with hookmngr.hook_all_context() + torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myconv output shape: torch.Size([1, 64, 112, 112])\n",
      "myconv input mean = 0.54, output mean = 0.00\n",
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n",
      "mylayer input mean = 0.51, output mean = 0.92\n"
     ]
    }
   ],
   "source": [
    "# lets only activate our forward hooks\n",
    "with hookmngr.hook_all_context(category='forward_hook') + torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set myconv input to zero\n",
      "myconv output shape: torch.Size([1, 64, 112, 112])\n",
      "Set mylayer input to zero\n",
      "mylayer output shape: torch.Size([1, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# now lets use only our original hook function, print_shape\n",
    "with hookmngr.hook_all_context(hook_types=[print_shape, zero_input]) + torch.no_grad():\n",
    "    resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward hooks\n",
    "def print_grad_shape_new(module, grad_in, grad_out):\n",
    "    module.val = 'val'\n",
    "    print('%s grad_in shape: ' % module.name, end='')\n",
    "    print(grad_in[0].shape, end=', ')\n",
    "    print('grad_out shape: ', grad_out.shape)\n",
    "\n",
    "hookmngr.register_backward_hook(print_grad_shape_new, resnet.conv1, hook_fn_name='print_grad_shape_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HookManager._forward_hook_base[myconv]': <HookManager._forward_hook_base[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (inactive)>,\n",
       " 'HookManager._forward_pre_hook_base[myconv]': <HookManager._forward_pre_hook_base[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (inactive)>,\n",
       " 'print_shape[myconv]': <print_shape[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (inactive)>,\n",
       " 'HookManager._forward_hook_base[mylayer]': <HookManager._forward_hook_base[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>,\n",
       " 'HookManager._forward_pre_hook_base[mylayer]': <HookManager._forward_pre_hook_base[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>,\n",
       " 'print_shape[mylayer]': <print_shape[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>,\n",
       " 'zero_input[myconv]': <zero_input[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (inactive)>,\n",
       " 'zero_input[mylayer]': <zero_input[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>,\n",
       " 'print_mean[myconv]': <print_mean[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (inactive)>,\n",
       " 'print_mean[mylayer]': <print_mean[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (inactive)>,\n",
       " 'print_grad_shape_new[myconv]': <print_grad_shape_new[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (active)>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hookmngr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = nn.CrossEntropyLoss()\n",
    "out = resnet(x)\n",
    "loss = xent(out, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = resnet.conv1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookmngr.register_backward_hook(print_grad_shape_new, resnet.layer1, hook_fn_name='print_grad_shape_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<print_grad_shape_new[myconv] <class 'tacklebox.hook_management.HookHandle'> registered to myconv (active)>,\n",
       " <print_grad_shape_new[mylayer] <class 'tacklebox.hook_management.HookHandle'> registered to mylayer (active)>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hookmngr.name_to_hookfn['print_grad_shape_new'].handles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In summary,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM - Multi30k machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = spacy.load('en')\n",
    "german = spacy.load('de')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in english.tokenizer(text)]\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in german.tokenizer(text)]\n",
    "\n",
    "en_text = Field(sequential=True, use_vocab=True, tokenize=tokenize_en, lower=True)\n",
    "de_text = Field(sequential=True, use_vocab=True, tokenize=tokenize_de, lower=True)\n",
    "\n",
    "train, val, test = Multi30k.splits(root='../data', exts=('.en', '.de'), fields=(en_text, de_text))\n",
    "\n",
    "en_text.build_vocab(train, max_size=30000, min_freq=3)\n",
    "de_text.build_vocab(train, max_size=30000, min_freq=3)\n",
    "vocab_en = en_text.vocab\n",
    "vocab_de = de_text.vocab\n",
    "pad_idx = vocab_de.stoi['<pad>']\n",
    "\n",
    "train_ldr, val_ldr, test_ldr = BucketIterator.splits((train, val, test),\n",
    "                                                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMWithLMHeadModel(\n",
       "  (transformer): XLMModel(\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (lang_embeddings): Embedding(2, 1024)\n",
       "    (embeddings): Embedding(4554, 1024, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pred_layer): XLMPredLayer(\n",
       "    (proj): Linear(in_features=1024, out_features=5374, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm = XLMWithLMHeadModel.from_pretrained('xlm-mlm-ende-1024')\n",
    "xlm.transformer.embeddings = nn.Embedding(len(vocab_en), xlm.config.emb_dim, padding_idx=pad_idx)\n",
    "xlm.pred_layer.proj = nn.Linear(xlm.config.emb_dim, len(vocab_de), bias=True)\n",
    "xlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = nn.CrossEntropyLoss()\n",
    "\n",
    "batch = next(iter(train_ldr))\n",
    "src, trg = batch.src.to(0), batch.trg.to(0)\n",
    "out, = xlm(src)\n",
    "min_idx = min([out.shape[0], trg.shape[0]])\n",
    "out, trg = out[:min_idx], trg[:min_idx]\n",
    "\n",
    "mask = (trg != pad_idx).type(torch.bool)\n",
    "loss = xent(out[mask], trg[mask])\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
