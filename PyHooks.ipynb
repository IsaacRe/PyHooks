{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18 - CIFAR100 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "from hook_management import HookManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = CIFAR100('../data/', train=True, transform=Compose([Resize((224, 224)), ToTensor()]))\n",
    "cifar_ldr = DataLoader(cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = resnet18()\n",
    "resnet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM - Multi30k machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = spacy.load('en')\n",
    "german = spacy.load('de')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in english.tokenizer(text)]\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in german.tokenizer(text)]\n",
    "\n",
    "en_text = Field(sequential=True, use_vocab=True, tokenize=tokenize_en, lower=True)\n",
    "de_text = Field(sequential=True, use_vocab=True, tokenize=tokenize_de, lower=True)\n",
    "\n",
    "train, val, test = Multi30k.splits(root='../data', exts=('.en', '.de'), fields=(en_text, de_text))\n",
    "\n",
    "en_text.build_vocab(train, max_size=30000, min_freq=3)\n",
    "de_text.build_vocab(train, max_size=30000, min_freq=3)\n",
    "vocab_en = en_text.vocab\n",
    "vocab_de = de_text.vocab\n",
    "pad_idx = vocab_de.stoi['<pad>']\n",
    "\n",
    "train_ldr, val_ldr, test_ldr = BucketIterator.splits((train, val, test),\n",
    "                                                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMWithLMHeadModel(\n",
       "  (transformer): XLMModel(\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (lang_embeddings): Embedding(2, 1024)\n",
       "    (embeddings): Embedding(4554, 1024, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pred_layer): XLMPredLayer(\n",
       "    (proj): Linear(in_features=1024, out_features=5374, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm = XLMWithLMHeadModel.from_pretrained('xlm-mlm-ende-1024')\n",
    "xlm.transformer.embeddings = nn.Embedding(len(vocab_en), xlm.config.emb_dim, padding_idx=pad_idx)\n",
    "xlm.pred_layer.proj = nn.Linear(xlm.config.emb_dim, len(vocab_de), bias=True)\n",
    "xlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = nn.CrossEntropyLoss()\n",
    "\n",
    "batch = next(iter(train_ldr))\n",
    "src, trg = batch.src.to(0), batch.trg.to(0)\n",
    "out, = xlm(src)\n",
    "min_idx = min([out.shape[0], trg.shape[0]])\n",
    "out, trg = out[:min_idx], trg[:min_idx]\n",
    "\n",
    "mask = (trg != pad_idx).type(torch.bool)\n",
    "loss = xent(out[mask], trg[mask])\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
